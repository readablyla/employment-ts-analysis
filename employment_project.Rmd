---
title: "Employment Analysis"
author: "Leala Darby, Scott Howard, Georgia Fardell"
date: "08/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
First load all required packages:
```{r message=FALSE, warning=FALSE}
library(car)
library(tseries)
library(astsa)
```

Load in the data:
```{r}
dat <- read.csv("employment_data.csv", fileEncoding = 'UTF-8-BOM')
head(dat)
```

Create a time series object from the data and plot. The blue lines are visually detected structural breakpoints - contextual reasoning is PC surge in the 90s and COVID-19. The red line indicates the training/test split. 
```{r}
ts_dat_test <- ts(dat[, 2], start = c(1978, 2), end = c(2020, 8), frequency = 12)
plot.ts(ts_dat_test, ylab="People Employed (000's)") #updated units
abline(v = 1993, col = "blue")
abline(v = 2020, col = "blue")
abline(v = 2019, col = "red", lty = 2)
```
We should use this plot in the intro section for our intitial time series.  

Instructed to truncate data from January 1993 to December 2019 (inclusive)
```{r}
dat[dat$Observation.times == "Jan-93",]
dat[dat$Observation.times == "Dec-19",]
dat[dat$Observation.times == "Jan-19",]
```
So we only need rows 180-503 for the total data set.
Splitting into training and test sets where the test set is all of 2019, we have train [180:491] and test [492:503].

```{r}
trunc_dat <- dat[180:503,] # all data after truncating
train_dat <- dat[180:491,] # training data
test_dat <- dat[492:503,] # test data
join_dat <- dat[491:492,] # the month between training data and predictions (for plotting)
ts_dat <- ts(train_dat[, 2], start = c(1993, 1), end = c(2018, 12), frequency = 12) # ts for model fitting
test_ts <- ts(test_dat[, 2], start = c(2019, 1), end = c(2019, 12), frequency = 12) # ts for model testing
ts_join <- ts(join_dat[, 2], start = c(2018, 12), end = c(2019, 1), frequency = 12) # between train and test
plot.ts(ts_dat, ylab="People Employed (000's)")
plot.ts(diff(ts_dat)) # We are not actually taking the difference yet!
# This 2nd plot is just to help observe trends in variance. 
```
The trend in mean is readily observable.
Difficult to determine a trend in variance - there appears to be frequent changes, which are easier to see after incorporating lags of 1.
Check statistically for stationarity using the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, which has the following hypotheses:
\begin{align*}
H_o&: \text{TS is stationary}\\
H_a&: \text{TS in not stationary}
\end{align*}
```{r}
kpss.test(ts_dat)
```
The small p-value indicates that we should reject the null and conclude that the ts is not stationary.

As a rough test of constant variance (Levene's isn't really valid because time series data isn't independent)
```{r}
length(ts_dat)
Group <- c(rep(1,78), rep(2, 78), rep(3, 78), rep(4, 78))
leveneTest(ts_dat, Group)
```
The small p-value of $0.04837$ confirms that the data exhibits heteroscedasticity. Therefore we will perform a log transformation to attempt to reduce this:
```{r}
log_ts_dat <- log(ts_dat)
plot.ts(cbind(ts_dat, log_ts_dat))
leveneTest(log_ts_dat, Group)
```

At a significance level of 5%, the p-value above of 0.7209 provides very weak evidence and we fail to reject the null hypothesis of equal variance among groups. Thus the heteroscedasticity has been reduced.

Next, to reduce the trend in mean, apply differencing of 1 lag to our TS with stabilised variance:
```{r}
f_ts_dat <- diff(log_ts_dat, 1)
plot.ts(cbind(ts_dat, log_ts_dat, f_ts_dat))
```
To confirm constant mean and variance and a Gaussian distribution for the time series, a Shapiro-Wilk normality test is performed:
```{r}
hist(f_ts_dat)
shapiro.test(f_ts_dat)
```
The small p-value indicates likely non-normality, but this test isn't really valid for TS. Instead, check statistically for stationarity using the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test:
```{r}
kpss.test(log_ts_dat)
kpss.test(f_ts_dat)
```
The final ts has a high p-value of 0.1, which is statistically significant at a significance level of 5%. Therefore we fail to reject the null hypothesis, and have reasonable evidence that the final ts is stationary.

Next, the ACF and PACF of the differenced ts are plotted for analysis. 
```{r}
acf2(f_ts_dat)
```
Seasonal patterns in the ACF for TS_name (Figure [above]) show a slow decay in the dominant lags. To mitigate this, the time series was differenced in 12 lags giving [differenced model] for which the ACF and PACF are plotted in Figure [below].  
```{r}
ts_dat_12 <- diff(f_ts_dat, 12)
kpss.test(ts_dat_12) #Big enough to call stationary
acf2(ts_dat_12)
```

At the seasonal level, these indicate a cutoff at 1 in the ACF and tailing off in PACF, possibly indicating P=0 and Q=1. Lags 1,2,â€¦,11 suggest several choices, so estimates of $0 \leq p \leq 1$ and $0 \leq q \leq 1$ are made and explored.  

```{r}
sarima(log_ts_dat, p = 1, d = 1, q = 1, P = 1, D = 1, Q = 1, S = 12) #AICc -8.123946
# ttable says ma1 coeff has highest p-value. removing this (model trimming):
sarima(log_ts_dat, p = 1, d = 1, q = 0, P = 1, D = 1, Q = 1, S = 12) #AICc -8.13055
# ttable says sar1 coeff has highest p-value. removing this:
sarima(log_ts_dat, p = 1, d = 1, q = 0, P = 0, D = 1, Q = 1, S = 12) #AICc -8.133259
```
For the first model, the Ljung-Box statistic is not satisfactory at lag 20, and unnesssecary coefficients were present in the model.
By trimming coefficients and finding the minimum bias-corrected AIC, the model selected was $SARIMA(1,1,0)(0,1,1)_{12}$.

We see a couple of outliers - pinpoint what these points are. The Ljung-Box statistic is passable at lag 20 or 30.  


Fit the model from above with lowest AIC
```{r}
fit <- arima(log_ts_dat, c(1,1,0), seasonal = list(order = c(0,1,1), period = 12))
fore <- predict(fit, n.ahead = 12)
```


Display predictions:
```{r}
ts.plot(cbind(ts_dat, exp(fore$pred)), lwd = c(2, 2), col = c(1, 2), xlim = c(2017, 2020), 
        ylab = "People Employed (000's)", ylim = c(11900, 13500))
lines(exp(fore$pred), type = "p", col = 2, lwd = 2)
lines(ts_join, lty = "dashed", lwd = 2)
lines(test_ts, lty = "dashed", lwd = 2) 
# 95% confidence boundaries 
lines(exp(fore$pred+2*fore$se), lty="dashed", col = 4, lwd = 2)
lines(exp(fore$pred-2*fore$se), lty="dashed", col = 4, lwd = 2)
```


Assess the predictions. MAPE etc.
test_ts is defined with the other data set breaks
```{r}
MAPE = 0
for (i in 1:12){
  MAPE = MAPE + abs((test_ts[i] - exp(fore$pred[i])) / test_ts[i]) 
}
MAPE = MAPE/12*100
MAPE
```
Discuss future work. Mention any TS that might be interesting, we could include a scatterplot if we have a candidate that shows good results?


